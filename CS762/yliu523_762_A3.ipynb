{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "textile-longer",
   "metadata": {},
   "source": [
    "CS 762 A3 - yliu523\n",
    "\n",
    "Data Preprocessing: \n",
    "There are no missing data in the trg.csv. In the preprocessing stage common stopwords as well as all punctuations have been removed for better training accuracy. All abstracts have been transformed into lowercase, as well as manual stemming of popular suffixes have been implemented. These methods were chosen based on my research for industry standard text preprocessing implementations.\n",
    "\n",
    "Implementation:\n",
    "Log probabilities - Instead of calculating the fractional conditional probabilities, we are instead calculating the sum of log probabilities for the word. This is because we are dealing with a large amount of words per paragraph and a small fractional value for each word. When these small fractions are multiplied together hundreds of times we may experience underflow - the value can get so small that python recognizes it as zero. Taking a log of this fraction preserves the propotionality, so our inference of maximum posterior probability still works the same way.\n",
    "\n",
    "Word stemming: We observe multiple versions of the same words (genes vs gene), so all words that end in 's' had their final letter removed as part of pre-processing. This should prevent the split in distribution between plural words that share the same meaning without affecting words that naturally end in 's' as their porportions will remain the same.\n",
    "\n",
    "Extensive stop-words: Originally I had a list of only 20 stopwords. During my implementation I found a more extensive list and wrote a python script to import the list to use in my own implementation. This increased the prediction accuracy slightly.\n",
    "\n",
    "Conclusion:\n",
    "A few different implementations were experimented with, however the base version that considered all words with extensive stopword list yieldeded the best results. I was unable to optimize TF-IDF runtime with my current structure and thus was only able to submit the base version of naive-bayes.\n",
    "\n",
    "Due to time constraint oversights I was only able to make an unsuccessful attempt at unsuccessful cross-validation and advanced metrics for measuring success.  \n",
    "\n",
    "Overall model accuracy - test set: 83.66%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "considered-bishop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import string\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "with open('trg.csv', 'r') as file:\n",
    "    data_iter=csv.reader(file,delimiter=',')\n",
    "    next(data_iter)\n",
    "    data=[data for data in data_iter]\n",
    "   \n",
    "data_array = np.asarray(data)\n",
    "\n",
    "processed={}\n",
    "\n",
    "# list of common english words\n",
    "common_words= ['able', 'about', 'above', 'abroad', 'according', 'accordingly', 'across', 'actually', 'adj', 'after', 'afterwards', 'again', 'against', 'ago', 'ahead', \"ain't\", 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'alongside', 'already', 'also', 'although', 'always', 'am', 'amid', 'amidst', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'appear', 'appreciate', 'appropriate', 'are', \"aren't\", 'around', 'as', \"a's\", 'aside', 'ask', 'asking', 'associated', 'at', 'available', 'away', 'awfully', 'back', 'backward', 'backwards', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'begin', 'behind', 'being', 'believe', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'both', 'brief', 'but', 'by', 'came', 'can', 'cannot', 'cant', \"can't\", 'caption', 'cause', 'causes', 'certain', 'certainly', 'changes', 'clearly', \"c'mon\", 'co', 'co.', 'com', 'come', 'comes', 'concerning', 'consequently', 'consider', 'considering', 'contain', 'containing', 'contains', 'corresponding', 'could', \"couldn't\", 'course', \"c's\", 'currently', 'dare', \"daren't\", 'definitely', 'described', 'despite', 'did', \"didn't\", 'different', 'directly', 'do', 'does', \"doesn't\", 'doing', 'done', \"don't\", 'down', 'downwards', 'during', 'each', 'edu', 'eg', 'eight', 'eighty', 'either', 'else', 'elsewhere', 'end', 'ending', 'enough', 'entirely', 'especially', 'et', 'etc', 'even', 'ever', 'evermore', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'exactly', 'example', 'except', 'fairly', 'far', 'farther', 'few', 'fewer', 'fifth', 'first', 'five', 'followed', 'following', 'follows', 'for', 'forever', 'former', 'formerly', 'forth', 'forward', 'found', 'four', 'from', 'further', 'furthermore', 'get', 'gets', 'getting', 'given', 'gives', 'go', 'goes', 'going', 'gone', 'got', 'gotten', 'greetings', 'had', \"hadn't\", 'half', 'happens', 'hardly', 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'hello', 'help', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', \"here's\", 'hereupon', 'hers', 'herself', \"he's\", 'hi', 'him', 'himself', 'his', 'hither', 'hopefully', 'how', 'howbeit', 'however', 'hundred', \"i'd\", 'ie', 'if', 'ignored', \"i'll\", \"i'm\", 'immediate', 'in', 'inasmuch', 'inc', 'inc.', 'indeed', 'indicate', 'indicated', 'indicates', 'inner', 'inside', 'insofar', 'instead', 'into', 'inward', 'is', \"isn't\", 'it', \"it'd\", \"it'll\", 'its', \"it's\", 'itself', \"i've\", 'just', 'k', 'keep', 'keeps', 'kept', 'know', 'known', 'knows', 'last', 'lately', 'later', 'latter', 'latterly', 'least', 'less', 'lest', 'let', \"let's\", 'like', 'liked', 'likely', 'likewise', 'little', 'look', 'looking', 'looks', 'low', 'lower', 'ltd', 'made', 'mainly', 'make', 'makes', 'many', 'may', 'maybe', \"mayn't\", 'me', 'mean', 'meantime', 'meanwhile', 'merely', 'might', \"mightn't\", 'mine', 'minus', 'miss', 'more', 'moreover', 'most', 'mostly', 'mr', 'mrs', 'much', 'must', \"mustn't\", 'my', 'myself', 'name', 'namely', 'nd', 'near', 'nearly', 'necessary', 'need', \"needn't\", 'needs', 'neither', 'never', 'neverf', 'neverless', 'nevertheless', 'new', 'next', 'nine', 'ninety', 'no', 'nobody', 'non', 'none', 'nonetheless', 'noone', 'no-one', 'nor', 'normally', 'not', 'nothing', 'notwithstanding', 'novel', 'now', 'nowhere', 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'on', 'once', 'one', 'ones', \"one's\", 'only', 'onto', 'opposite', 'or', 'other', 'others', 'otherwise', 'ought', \"oughtn't\", 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'own', 'particular', 'particularly', 'past', 'per', 'perhaps', 'placed', 'please', 'plus', 'possible', 'presumably', 'probably', 'provided', 'provides', 'que', 'quite', 'qv', 'rather', 'rd', 're', 'really', 'reasonably', 'recent', 'recently', 'regarding', 'regardless', 'regards', 'relatively', 'respectively', 'right', 'round', 'said', 'same', 'saw', 'say', 'saying', 'says', 'second', 'secondly', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'several', 'shall', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'since', 'six', 'so', 'some', 'somebody', 'someday', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'sorry', 'specified', 'specify', 'specifying', 'still', 'sub', 'such', 'sup', 'sure', 'take', 'taken', 'taking', 'tell', 'tends', 'th', 'than', 'thank', 'thanks', 'thanx', 'that', \"that'll\", 'thats', \"that's\", \"that've\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', \"there'd\", 'therefore', 'therein', \"there'll\", \"there're\", 'theres', \"there's\", 'thereupon', \"there've\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'thing', 'things', 'think', 'third', 'thirty', 'this', 'thorough', 'thoroughly', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'till', 'to', 'together', 'too', 'took', 'toward', 'towards', 'tried', 'tries', 'truly', 'try', 'trying', \"t's\", 'twice', 'two', 'un', 'under', 'underneath', 'undoing', 'unfortunately', 'unless', 'unlike', 'unlikely', 'until', 'unto', 'up', 'upon', 'upwards', 'us', 'use', 'used', 'useful', 'uses', 'using', 'usually', 'v', 'value', 'various', 'versus', 'very', 'via', 'viz', 'vs', 'want', 'wants', 'was', \"wasn't\", 'way', 'we', \"we'd\", 'welcome', 'well', \"we'll\", 'went', 'were', \"we're\", \"weren't\", \"we've\", 'what', 'whatever', \"what'll\", \"what's\", \"what've\", 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', \"where's\", 'whereupon', 'wherever', 'whether', 'which', 'whichever', 'while', 'whilst', 'whither', 'who', \"who'd\", 'whoever', 'whole', \"who'll\", 'whom', 'whomever', \"who's\", 'whose', 'why', 'will', 'willing', 'wish', 'with', 'within', 'without', 'wonder', \"won't\", 'would', \"wouldn't\", 'yes', 'yet', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\", 'zero', 'a', \"how's\", 'i', \"when's\", \"why's\", 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'j', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'uucp', 'w', 'x', 'y', 'z', 'I', 'www', 'amount', 'bill', 'bottom', 'call', 'computer', 'con', 'couldnt', 'cry', 'de', 'describe', 'detail', 'due', 'eleven', 'empty', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'forty', 'front', 'full', 'give', 'hasnt', 'herse', 'himse', 'interest', 'itse”', 'mill', 'move', 'myse”', 'part', 'put', 'show', 'side', 'sincere', 'sixty', 'system', 'ten', 'thick', 'thin', 'top', 'twelve', 'twenty', 'abst', 'accordance', 'act', 'added', 'adopted', 'affected', 'affecting', 'affects', 'ah', 'announce', 'anymore', 'apparently', 'approximately', 'aren', 'arent', 'arise', 'auth', 'beginning', 'beginnings', 'begins', 'biol', 'briefly', 'ca', 'date', 'ed', 'effect', 'et-al', 'ff', 'fix', 'gave', 'giving', 'heres', 'hes', 'hid', 'home', 'id', 'im', 'immediately', 'importance', 'important', 'index', 'information', 'invention', 'itd', 'keys', 'kg', 'km', 'largely', 'lets', 'line', \"'ll\", 'means', 'mg', 'million', 'ml', 'mug', 'na', 'nay', 'necessarily', 'nos', 'noted', 'obtain', 'obtained', 'omitted', 'ord', 'owing', 'page', 'pages', 'poorly', 'possibly', 'potentially', 'pp', 'predominantly', 'present', 'previously', 'primarily', 'promptly', 'proud', 'quickly', 'ran', 'readily', 'ref', 'refs', 'related', 'research', 'resulted', 'resulting', 'results', 'run', 'sec', 'section', 'shed', 'shes', 'showed', 'shown', 'showns', 'shows', 'significant', 'significantly', 'similar', 'similarly', 'slightly', 'somethan', 'specifically', 'state', 'states', 'stop', 'strongly', 'substantially', 'successfully', 'sufficiently', 'suggest', 'thered', 'thereof', 'therere', 'thereto', 'theyd', 'theyre', 'thou', 'thoughh', 'thousand', 'throug', 'til', 'tip', 'ts', 'ups', 'usefully', 'usefulness', \"'ve\", 'vol', 'vols', 'wed', 'whats', 'wheres', 'whim', 'whod', 'whos', 'widely', 'words', 'world', 'youd', 'youre']\n",
    "\n",
    "\n",
    "\n",
    "for abstract in data_array: # use [1:21] to get first 20 \n",
    "    classifier = abstract[1]\n",
    "    paragraph = abstract[2].translate(str.maketrans('','',string.punctuation)).lower()\n",
    "    p= paragraph.split()\n",
    "    clean_para= []\n",
    "    for word in p:\n",
    "        if word not in common_words:\n",
    "            clean_para.append(word.removesuffix('s'))\n",
    "           \n",
    "    if classifier not in processed.keys():\n",
    "        processed[classifier]=[]\n",
    "        processed[classifier].append(clean_para)\n",
    "    else:\n",
    "        processed[classifier].append(clean_para)\n",
    "\n",
    "        \n",
    "#processed dictionary is created at end of this block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "solar-testing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n",
      "A\n",
      "E\n",
      "V\n",
      "prelaplace vs prelaplace + laplace const\n",
      "[168635, 16870, 232633, 15301]\n",
      "{'B': {'B_prior': -0.9150415124737231, 'B_empty': -12.193706498942605}, 'A': {'A_prior': -3.4420193761824103, 'A_empty': -10.731537060559845}, 'E': {'E_prior': -0.623621117911335, 'E_empty': -12.474342514460494}, 'V': {'V_prior': -3.4577677331505496, 'V_empty': -10.696661047163198}}\n"
     ]
    }
   ],
   "source": [
    "def NaiveBayes(processed):\n",
    "    structure = {}\n",
    "    struc_const = {}\n",
    "    all_w = []\n",
    "    all_w_dupe = []\n",
    "    pre_laplace = []\n",
    "    prior = []\n",
    "\n",
    "#     sparse_wc = []\n",
    "\n",
    "    for i, j in processed.items():\n",
    "        for x in processed[i]:\n",
    "            all_w = all_w + x\n",
    "    \n",
    "    all_w_uniq = np.unique(all_w)\n",
    "\n",
    "    for key in processed: # a b c d looping\n",
    "        class_word_dupe = []\n",
    "        enum = []\n",
    "        \n",
    "#         term_freq = []\n",
    "#         idf_list = []\n",
    "        \n",
    "        class_wordcounts = {}\n",
    "        print(key)\n",
    "        \n",
    "        for para in processed[key]:\n",
    "            class_word_dupe = class_word_dupe + para\n",
    "        \n",
    "        class_word_uniq = np.unique(class_word_dupe)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for w in class_word_uniq: # for every unique word in this class, count how many times it appears, then add laplace constant\n",
    "            n = class_word_dupe.count(w) + 1\n",
    "            enum.append(n)  \n",
    "            \n",
    "        class_wordcounts = dict(zip(class_word_uniq, enum)) # zip the number of occurence and the unique words to form a dictionary.\n",
    "    \n",
    "        \n",
    "#         without_sparse = {i:j for i,j in class_wordcounts.items() if j > 50}\n",
    "#         print('sparsity check')\n",
    "#         print(len(class_wordcounts))\n",
    "#         print(len(without_sparse))\n",
    "\n",
    "        if key not in structure.keys(): # Initialize the final output (structure)\n",
    "            structure[key] = class_wordcounts\n",
    "            \n",
    "        words_in_class = sum(class_wordcounts.values())\n",
    "        \n",
    "        pre_laplace.append(words_in_class)\n",
    "        \n",
    "    print('prelaplace vs prelaplace + laplace const')\n",
    "    print(pre_laplace)\n",
    "    \n",
    "    laplace_con = len(all_w_uniq) # count of unique words in CSV\n",
    "\n",
    "    denom = [x+laplace_con for x in pre_laplace]\n",
    "    \n",
    "    # PRIOR CALCULATION START\n",
    "    all_row = 0\n",
    "    row_in_class = []\n",
    "    for key in processed:\n",
    "        row_in_class.append(len(processed[key]))\n",
    "        all_row += len(processed[key])\n",
    "\n",
    "    prior_list = [x / all_row for x in row_in_class]\n",
    "    \n",
    "    z=0\n",
    "    for key in processed:\n",
    "        if key not in struc_const:\n",
    "            struc_const[key] = {key + '_prior' : math.log(prior_list[z]),\n",
    "                                key + '_empty' : math.log(1/denom[z])}\n",
    "        z += 1\n",
    "        \n",
    "    # prior values and empty values\n",
    "    print(struc_const)\n",
    "    \n",
    "    c = 0\n",
    "    for i,j in structure.items():\n",
    "        for k,l in j.items():\n",
    "            struc_const[i][k]= math.log(l/denom[c]) # divide each whole wordcount (7) by denominator (7/1500), store the fractional/decimal\n",
    "        c += 1\n",
    "        \n",
    "#   print('Structure AFTER decimal and LOG calculations')\n",
    "    return(struc_const)\n",
    "\n",
    "    \n",
    "task1 = NaiveBayes(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "spread-anderson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in a test file\n",
    "def read_tst(file):\n",
    "    with open(file, 'r') as destination:\n",
    "        data_iterator = csv.reader(destination, delimiter=',')\n",
    "        data=[data[1].translate(str.maketrans('','',string.punctuation)).lower() for data in data_iterator][1:]\n",
    "        \n",
    "    for i in range(len(data)):\n",
    "        data[i]=data[i].split()\n",
    "        \n",
    "    return data\n",
    "\n",
    "tst = read_tst(\"tst.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "consolidated-style",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-initialize the prior valuwes\n",
    "priors = [0,0,0,0]\n",
    "\n",
    "for i, j in task1.items():\n",
    "    if i == 'A':\n",
    "        priors[0] += j['A_prior']\n",
    "    if i == 'B':\n",
    "        priors[1] += j['B_prior']\n",
    "    if i == 'E':\n",
    "        priors[2] += j['E_prior']\n",
    "    if i == 'V':\n",
    "        priors[3] += j['V_prior']\n",
    "\n",
    "# print(priors)\n",
    "\n",
    "# MAKE PREDICTION, sentence = a row, pred_structure = log prob dictionary.\n",
    "def predict(sentence, pred_structure):\n",
    "\n",
    "    class_name = ['A','B','E','V']\n",
    "    class_prob = [0  ,0  ,0  ,0  ]\n",
    "    \n",
    "    for word in sentence:\n",
    "        for i, j in pred_structure.items():  \n",
    "            if word in j.keys():\n",
    "                class_prob[class_name.index(i)] += j[word]\n",
    "                \n",
    "    for i in range(4):\n",
    "        class_prob[i] =  priors[i] + class_prob[i] \n",
    "    final_prediction = class_name[class_prob.index(min(class_prob))]\n",
    "    \n",
    "    return(final_prediction) # return either A B E or V\n",
    "\n",
    "\n",
    "final_counts = []\n",
    "\n",
    "# tst is test data, it should have 1000 rows, for every row, run predict and append the letter to empty list we inited before.\n",
    "for para in tst:\n",
    "    final_counts.append(predict(para, task1))\n",
    "\n",
    "\n",
    "# print(final_counts)\n",
    "\n",
    "# print(final_counts.count('A'))\n",
    "# print(final_counts.count('B'))\n",
    "# print(final_counts.count('E'))\n",
    "# print(final_counts.count('V'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fifty-grocery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results from above into a CSV file\n",
    "\n",
    "def print_to_csv(pred_list): # self explanatory\n",
    "    int = 0\n",
    "    file = open('yliu523.csv', 'a') \n",
    "    file.write('id,class'+'\\n')  \n",
    "    for pred in pred_list: \n",
    "        int += 1\n",
    "        file.write(str(int)+','+pred+'\\n')  \n",
    "    file.close() \n",
    "\n",
    "print_to_csv(final_counts) # This creates the file for kaggle handin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "divine-purpose",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-57e45f0ed7a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trg.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-57e45f0ed7a5>\u001b[0m in \u001b[0;36mCV\u001b[0;34m(trg)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-57e45f0ed7a5>\u001b[0m in \u001b[0;36mxFold\u001b[0;34m(data, i, folds)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "# cross validation code:\n",
    "import random\n",
    "\n",
    "def CV(trg):\n",
    "    accuracy = []\n",
    "\n",
    "    with open(trg, 'r') as file:\n",
    "        data_iter=csv.reader(file,delimiter=',')\n",
    "        next(data_iter)\n",
    "        data=[data for data in data_iter]\n",
    "\n",
    "    data_array = np.asarray(data)\n",
    "    processed={}\n",
    "\n",
    "    # list of common english words\n",
    "    common_words= ['able', 'about', 'above', 'abroad', 'according', 'accordingly', 'across', 'actually', 'adj', 'after', 'afterwards', 'again', 'against', 'ago', 'ahead', \"ain't\", 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'alongside', 'already', 'also', 'although', 'always', 'am', 'amid', 'amidst', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'appear', 'appreciate', 'appropriate', 'are', \"aren't\", 'around', 'as', \"a's\", 'aside', 'ask', 'asking', 'associated', 'at', 'available', 'away', 'awfully', 'back', 'backward', 'backwards', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'begin', 'behind', 'being', 'believe', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'both', 'brief', 'but', 'by', 'came', 'can', 'cannot', 'cant', \"can't\", 'caption', 'cause', 'causes', 'certain', 'certainly', 'changes', 'clearly', \"c'mon\", 'co', 'co.', 'com', 'come', 'comes', 'concerning', 'consequently', 'consider', 'considering', 'contain', 'containing', 'contains', 'corresponding', 'could', \"couldn't\", 'course', \"c's\", 'currently', 'dare', \"daren't\", 'definitely', 'described', 'despite', 'did', \"didn't\", 'different', 'directly', 'do', 'does', \"doesn't\", 'doing', 'done', \"don't\", 'down', 'downwards', 'during', 'each', 'edu', 'eg', 'eight', 'eighty', 'either', 'else', 'elsewhere', 'end', 'ending', 'enough', 'entirely', 'especially', 'et', 'etc', 'even', 'ever', 'evermore', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'exactly', 'example', 'except', 'fairly', 'far', 'farther', 'few', 'fewer', 'fifth', 'first', 'five', 'followed', 'following', 'follows', 'for', 'forever', 'former', 'formerly', 'forth', 'forward', 'found', 'four', 'from', 'further', 'furthermore', 'get', 'gets', 'getting', 'given', 'gives', 'go', 'goes', 'going', 'gone', 'got', 'gotten', 'greetings', 'had', \"hadn't\", 'half', 'happens', 'hardly', 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'hello', 'help', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', \"here's\", 'hereupon', 'hers', 'herself', \"he's\", 'hi', 'him', 'himself', 'his', 'hither', 'hopefully', 'how', 'howbeit', 'however', 'hundred', \"i'd\", 'ie', 'if', 'ignored', \"i'll\", \"i'm\", 'immediate', 'in', 'inasmuch', 'inc', 'inc.', 'indeed', 'indicate', 'indicated', 'indicates', 'inner', 'inside', 'insofar', 'instead', 'into', 'inward', 'is', \"isn't\", 'it', \"it'd\", \"it'll\", 'its', \"it's\", 'itself', \"i've\", 'just', 'k', 'keep', 'keeps', 'kept', 'know', 'known', 'knows', 'last', 'lately', 'later', 'latter', 'latterly', 'least', 'less', 'lest', 'let', \"let's\", 'like', 'liked', 'likely', 'likewise', 'little', 'look', 'looking', 'looks', 'low', 'lower', 'ltd', 'made', 'mainly', 'make', 'makes', 'many', 'may', 'maybe', \"mayn't\", 'me', 'mean', 'meantime', 'meanwhile', 'merely', 'might', \"mightn't\", 'mine', 'minus', 'miss', 'more', 'moreover', 'most', 'mostly', 'mr', 'mrs', 'much', 'must', \"mustn't\", 'my', 'myself', 'name', 'namely', 'nd', 'near', 'nearly', 'necessary', 'need', \"needn't\", 'needs', 'neither', 'never', 'neverf', 'neverless', 'nevertheless', 'new', 'next', 'nine', 'ninety', 'no', 'nobody', 'non', 'none', 'nonetheless', 'noone', 'no-one', 'nor', 'normally', 'not', 'nothing', 'notwithstanding', 'novel', 'now', 'nowhere', 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'on', 'once', 'one', 'ones', \"one's\", 'only', 'onto', 'opposite', 'or', 'other', 'others', 'otherwise', 'ought', \"oughtn't\", 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'own', 'particular', 'particularly', 'past', 'per', 'perhaps', 'placed', 'please', 'plus', 'possible', 'presumably', 'probably', 'provided', 'provides', 'que', 'quite', 'qv', 'rather', 'rd', 're', 'really', 'reasonably', 'recent', 'recently', 'regarding', 'regardless', 'regards', 'relatively', 'respectively', 'right', 'round', 'said', 'same', 'saw', 'say', 'saying', 'says', 'second', 'secondly', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'several', 'shall', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'since', 'six', 'so', 'some', 'somebody', 'someday', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'sorry', 'specified', 'specify', 'specifying', 'still', 'sub', 'such', 'sup', 'sure', 'take', 'taken', 'taking', 'tell', 'tends', 'th', 'than', 'thank', 'thanks', 'thanx', 'that', \"that'll\", 'thats', \"that's\", \"that've\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', \"there'd\", 'therefore', 'therein', \"there'll\", \"there're\", 'theres', \"there's\", 'thereupon', \"there've\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'thing', 'things', 'think', 'third', 'thirty', 'this', 'thorough', 'thoroughly', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'till', 'to', 'together', 'too', 'took', 'toward', 'towards', 'tried', 'tries', 'truly', 'try', 'trying', \"t's\", 'twice', 'two', 'un', 'under', 'underneath', 'undoing', 'unfortunately', 'unless', 'unlike', 'unlikely', 'until', 'unto', 'up', 'upon', 'upwards', 'us', 'use', 'used', 'useful', 'uses', 'using', 'usually', 'v', 'value', 'various', 'versus', 'very', 'via', 'viz', 'vs', 'want', 'wants', 'was', \"wasn't\", 'way', 'we', \"we'd\", 'welcome', 'well', \"we'll\", 'went', 'were', \"we're\", \"weren't\", \"we've\", 'what', 'whatever', \"what'll\", \"what's\", \"what've\", 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', \"where's\", 'whereupon', 'wherever', 'whether', 'which', 'whichever', 'while', 'whilst', 'whither', 'who', \"who'd\", 'whoever', 'whole', \"who'll\", 'whom', 'whomever', \"who's\", 'whose', 'why', 'will', 'willing', 'wish', 'with', 'within', 'without', 'wonder', \"won't\", 'would', \"wouldn't\", 'yes', 'yet', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\", 'zero', 'a', \"how's\", 'i', \"when's\", \"why's\", 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'j', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'uucp', 'w', 'x', 'y', 'z', 'I', 'www', 'amount', 'bill', 'bottom', 'call', 'computer', 'con', 'couldnt', 'cry', 'de', 'describe', 'detail', 'due', 'eleven', 'empty', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'forty', 'front', 'full', 'give', 'hasnt', 'herse', 'himse', 'interest', 'itse”', 'mill', 'move', 'myse”', 'part', 'put', 'show', 'side', 'sincere', 'sixty', 'system', 'ten', 'thick', 'thin', 'top', 'twelve', 'twenty', 'abst', 'accordance', 'act', 'added', 'adopted', 'affected', 'affecting', 'affects', 'ah', 'announce', 'anymore', 'apparently', 'approximately', 'aren', 'arent', 'arise', 'auth', 'beginning', 'beginnings', 'begins', 'biol', 'briefly', 'ca', 'date', 'ed', 'effect', 'et-al', 'ff', 'fix', 'gave', 'giving', 'heres', 'hes', 'hid', 'home', 'id', 'im', 'immediately', 'importance', 'important', 'index', 'information', 'invention', 'itd', 'keys', 'kg', 'km', 'largely', 'lets', 'line', \"'ll\", 'means', 'mg', 'million', 'ml', 'mug', 'na', 'nay', 'necessarily', 'nos', 'noted', 'obtain', 'obtained', 'omitted', 'ord', 'owing', 'page', 'pages', 'poorly', 'possibly', 'potentially', 'pp', 'predominantly', 'present', 'previously', 'primarily', 'promptly', 'proud', 'quickly', 'ran', 'readily', 'ref', 'refs', 'related', 'research', 'resulted', 'resulting', 'results', 'run', 'sec', 'section', 'shed', 'shes', 'showed', 'shown', 'showns', 'shows', 'significant', 'significantly', 'similar', 'similarly', 'slightly', 'somethan', 'specifically', 'state', 'states', 'stop', 'strongly', 'substantially', 'successfully', 'sufficiently', 'suggest', 'thered', 'thereof', 'therere', 'thereto', 'theyd', 'theyre', 'thou', 'thoughh', 'thousand', 'throug', 'til', 'tip', 'ts', 'ups', 'usefully', 'usefulness', \"'ve\", 'vol', 'vols', 'wed', 'whats', 'wheres', 'whim', 'whod', 'whos', 'widely', 'words', 'world', 'youd', 'youre']\n",
    "\n",
    "    for abstract in data_array: # use [1:21] to get first 20 \n",
    "        classifier = abstract[1]\n",
    "        paragraph = abstract[2].translate(str.maketrans('','',string.punctuation)).lower()\n",
    "        p= paragraph.split()\n",
    "        clean_para= []\n",
    "        for word in p:\n",
    "            if word not in common_words:\n",
    "                clean_para.append(word.removesuffix('s'))\n",
    "\n",
    "        if classifier not in processed.keys():\n",
    "            processed[classifier]=[]\n",
    "            processed[classifier].append(clean_para)\n",
    "        else:\n",
    "            processed[classifier].append(clean_para)\n",
    "            \n",
    "    acc = []\n",
    "    for i in range(10):\n",
    "        X, y = xFold(processed, i, folds = 10)\n",
    "        \n",
    "        X_train = X\n",
    "        y_train = [i[2].translate(str.maketrans('','',string.punctuation)).lower() for i in y]\n",
    "        \n",
    "        for i in range(len(y_train)):\n",
    "            y_train[i] = y_train[i].split()\n",
    "            \n",
    "        y_test = y[:,1]\n",
    "        \n",
    "        X_struc = naiveBayes(X_train)\n",
    "        \n",
    "        y_list = []\n",
    "        \n",
    "        for i in y_train:\n",
    "            y_list.append(predict(i, X_struc))\n",
    "        \n",
    "        acc.append(sum(1 for i,j in zip(y_test, y_list) if i == j)) / len(y_list)\n",
    "        \n",
    "    return acc\n",
    "        \n",
    "\n",
    "def xFold(data, i, folds):\n",
    "    np.random.seed()\n",
    "    \n",
    "    test = data[0::folds]\n",
    "    train = [i for i in data if i not in test]\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "\n",
    "test = CV('trg.csv')\n",
    "print(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-cemetery",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
